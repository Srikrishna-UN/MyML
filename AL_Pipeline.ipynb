{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The code might not be working due to the paths being of my local drive and in our college IDs we cant create shared drives\n",
        " -SK"
      ],
      "metadata": {
        "id": "bnyI3EX9Wi-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Nx0Q8TVMWlqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuring reproduciblity"
      ],
      "metadata": {
        "id": "IWWNtsa3h8ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random,numpy as np\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2TIk2-th_vv",
        "outputId": "212f5590-4fbd-4e61-9ed4-5142b44d6f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dcbd4079af0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the Data"
      ],
      "metadata": {
        "id": "jyY02j5dg4MX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_pPrx2_LLsz",
        "outputId": "6d27c1fe-78be-4143-f48f-5e5256582ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 207818288.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 30306653.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 60890581.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12959543.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Training set size: 60000\n",
            "Testing set size: 10000\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_set = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_set = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Print the sizes of the datasets\n",
        "print(\"Training set size:\", len(train_set))\n",
        "print(\"Testing set size:\", len(test_set))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "initial_train_set,rest = torch.utils.data.random_split(train_set,[10000,len(train_set)-10000])\n",
        "val_set,remainder = torch.utils.data.random_split(rest,[10000,len(rest)-10000])\n",
        "len(initial_train_set),len(rest),len(remainder),len(val_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csqC3x5uMrpw",
        "outputId": "da8077c3-5728-42b7-94ad-9004e56fd99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 50000, 40000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LeNet-5 model"
      ],
      "metadata": {
        "id": "l7SF47zBhHB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hB2yac4oSvED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet5()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYmIw6ANTIdg",
        "outputId": "bc4cf989-0bee-4a61-cb82-8f5635335f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LeNet5(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "mJvYOhh1hQRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataset, epochs, learning_rate):\n",
        "    train_loader = DataLoader(train_dataset,batch_size = 64)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss / len(train_loader):.3f}\")\n",
        "\n",
        "    print(\"Finished training\")\n",
        "\n",
        "# train_model(model,initial_train_set,10,0.01)"
      ],
      "metadata": {
        "id": "Is7HF5LuWcHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test loop"
      ],
      "metadata": {
        "id": "op4FQ2tThTDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_set):\n",
        "    test_loader = DataLoader(test_set,batch_size=64)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "test_model(model,test_set)"
      ],
      "metadata": {
        "id": "pfNIhu5UfXXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb2a220-30b5-42ae-a456-f67cd25f5b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.57"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and saving the the model with best accuracy"
      ],
      "metadata": {
        "id": "dqdQzH-wKtVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_save(train_dataset, test_dataset,n, epochs, lr):\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64)\n",
        "    best_accuracy = 0.0\n",
        "    model = LeNet5()\n",
        "    train_model(model, train_dataset, epochs, lr)\n",
        "    best_accuracy = test_model(model, test_set)\n",
        "    print(f\"Initial best accuracy:{best_accuracy:.2f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"/content/drive/MyDrive/MyML/Models/model_{best_accuracy:.2f}.pt\")\n",
        "\n",
        "    for _ in range(n):\n",
        "        model=LeNet5()\n",
        "        train_model(model, train_dataset, epochs, lr)\n",
        "        new_accuracy = test_model(model, test_set)\n",
        "        print(f\"New accuracy:{new_accuracy:.2f}\\nBest accuracy:{best_accuracy:.2f}\")\n",
        "        if new_accuracy > best_accuracy :\n",
        "            best_accuracy = new_accuracy\n",
        "            if new_accuracy > 95.00 :\n",
        "              torch.save(model.state_dict(), f\"/content/drive/MyDrive/MyML/Models/model_{best_accuracy:.2f}.pt\")\n",
        "\n",
        "train_test_save(initial_train_set, test_set, n=3, epochs=10, lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2yquFhOkffR",
        "outputId": "ee21af5b-4320-41a2-8623-4167d10cc0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 2.300\n",
            "Epoch [2/10] Loss: 2.290\n",
            "Epoch [3/10] Loss: 2.271\n",
            "Epoch [4/10] Loss: 2.207\n",
            "Epoch [5/10] Loss: 1.766\n",
            "Epoch [6/10] Loss: 0.792\n",
            "Epoch [7/10] Loss: 0.495\n",
            "Epoch [8/10] Loss: 0.387\n",
            "Epoch [9/10] Loss: 0.322\n",
            "Epoch [10/10] Loss: 0.277\n",
            "Finished training\n",
            "Initial best accuracy:92.20\n",
            "Epoch [1/10] Loss: 2.299\n",
            "Epoch [2/10] Loss: 2.278\n",
            "Epoch [3/10] Loss: 2.228\n",
            "Epoch [4/10] Loss: 1.902\n",
            "Epoch [5/10] Loss: 0.844\n",
            "Epoch [6/10] Loss: 0.465\n",
            "Epoch [7/10] Loss: 0.356\n",
            "Epoch [8/10] Loss: 0.299\n",
            "Epoch [9/10] Loss: 0.261\n",
            "Epoch [10/10] Loss: 0.233\n",
            "Finished training\n",
            "New accuracy:92.02\n",
            "Best accuracy:92.20\n",
            "Epoch [1/10] Loss: 2.301\n",
            "Epoch [2/10] Loss: 2.290\n",
            "Epoch [3/10] Loss: 2.271\n",
            "Epoch [4/10] Loss: 2.214\n",
            "Epoch [5/10] Loss: 1.804\n",
            "Epoch [6/10] Loss: 0.768\n",
            "Epoch [7/10] Loss: 0.462\n",
            "Epoch [8/10] Loss: 0.360\n",
            "Epoch [9/10] Loss: 0.307\n",
            "Epoch [10/10] Loss: 0.273\n",
            "Finished training\n",
            "New accuracy:91.22\n",
            "Best accuracy:92.20\n",
            "Epoch [1/10] Loss: 2.300\n",
            "Epoch [2/10] Loss: 2.289\n",
            "Epoch [3/10] Loss: 2.270\n",
            "Epoch [4/10] Loss: 2.204\n",
            "Epoch [5/10] Loss: 1.724\n",
            "Epoch [6/10] Loss: 0.785\n",
            "Epoch [7/10] Loss: 0.493\n",
            "Epoch [8/10] Loss: 0.382\n",
            "Epoch [9/10] Loss: 0.318\n",
            "Epoch [10/10] Loss: 0.275\n",
            "Finished training\n",
            "New accuracy:91.14\n",
            "Best accuracy:92.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving and loading the model with best accuracy"
      ],
      "metadata": {
        "id": "2JYDldxQK1oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/MyML/Models/\"\n",
        "\n",
        "def get_accuracy_from_filename(filename):\n",
        "    return float(filename.split(\"_\")[1][:-3])\n",
        "model_files = os.listdir(folder_path)\n",
        "model_files = [file for file in model_files if file.startswith(\"model_\") and file.endswith(\".pt\")]\n",
        "\n",
        "if not model_files:\n",
        "    print(\"No model files found in the folder.\")\n",
        "else:\n",
        "    # Find the model file with the highest accuracy\n",
        "    best_model_filename = max(model_files, key=get_accuracy_from_filename)\n",
        "    best_model_path = os.path.join(folder_path, best_model_filename)\n",
        "    best_model = LeNet5()\n",
        "    best_model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    print(f\"Loaded the model with the highest accuracy: {best_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVP6T4MKMHpW",
        "outputId": "cfdf31e4-ff50-4547-dd1c-36c5ba434a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the model with the highest accuracy: /content/drive/MyDrive/MyML/Models/model_96.57.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least Confidence and retraining"
      ],
      "metadata": {
        "id": "BvbNcKroLG5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "def least_confidence_images(model, test_dataset, k=5000):\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "    confidences = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            max_probs, _ = torch.max(probs, dim=1)\n",
        "            confidences.extend(max_probs.tolist())\n",
        "            labels.extend(targets.tolist())\n",
        "    confidences = torch.tensor(confidences)\n",
        "    _, indices = torch.topk(confidences, k, largest=False)\n",
        "    return Subset(test_dataset, indices), [labels[i] for i in indices]\n",
        "least_conf_images, least_conf_labels = least_confidence_images(model, remainder, k=5000)\n",
        "train_dataset = torch.utils.data.ConcatDataset([initial_train_set, least_conf_images])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cghdo5IBLdwe",
        "outputId": "f5a145be-e1fb-499c-888a-db7ee12ee847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_until_degradation(model, initial_train_set, remaining_set, val_set, batch_size, degradation_threshold=0.05):\n",
        "    # Initialize the training dataset with the initial subset\n",
        "    train_dataset = initial_train_set\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set,batch_size=64)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    # Initial evaluation on the validation set to calculate baseline accuracy\n",
        "    best_accuracy = test_model(model, val_set)\n",
        "    current_accuracy = best_accuracy\n",
        "    # Initialize the degradation flag\n",
        "    is_degraded = False\n",
        "    epoch = 0\n",
        "    max_epochs = 100  # Set the maximum number of epochs to 10\n",
        "    while not is_degraded and epoch < max_epochs:\n",
        "        epoch += 1\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        least_conf_images, _ = least_confidence_images(model, remaining_set, k=5000)\n",
        "        # Combine the least confident images with the current training dataset\n",
        "        train_dataset = torch.utils.data.ConcatDataset([train_dataset, least_conf_images])\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        current_accuracy = test_model(model, val_set)\n",
        "        print(f\"Epoch [{epoch}], Current accuracy: {current_accuracy:.2f}, Best accuracy: {best_accuracy:.2f}\")\n",
        "        # Check for improvement in accuracy\n",
        "        if current_accuracy > best_accuracy:\n",
        "            best_accuracy = current_accuracy\n",
        "        # Check for degradation\n",
        "        if (best_accuracy - current_accuracy) >= degradation_threshold:\n",
        "            is_degraded = True\n",
        "            print(\"Degradation threshold reached. Stopping training.\")\n",
        "    print(\"Final accuracy on validation set:\", current_accuracy)\n",
        "    torch.save(model, f\"/content/drive/MyDrive/MyML/Models/final_model_{current_accuracy}.pt\")\n",
        "    return model\n",
        "# new_model = train_until_degradation(model, initial_train_set, remainder, val_set, batch_size=64, degradation_threshold=0.05)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHdD0IJDLFzY",
        "outputId": "8f0c565f-5b87-4baa-fd0a-95c0cf44937d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], Current accuracy: 96.5900, Best accuracy: 96.3500\n",
            "Epoch [2], Current accuracy: 98.0300, Best accuracy: 96.5900\n",
            "Epoch [3], Current accuracy: 98.1100, Best accuracy: 98.0300\n",
            "Epoch [4], Current accuracy: 98.7100, Best accuracy: 98.1100\n",
            "Epoch [5], Current accuracy: 98.8100, Best accuracy: 98.7100\n",
            "Epoch [6], Current accuracy: 98.8600, Best accuracy: 98.8100\n",
            "Epoch [7], Current accuracy: 99.0600, Best accuracy: 98.8600\n",
            "Epoch [8], Current accuracy: 98.7500, Best accuracy: 99.0600\n",
            "Degradation threshold reached. Stopping training.\n",
            "Final accuracy on validation set: 98.75\n"
          ]
        }
      ]
    }
  ]
}